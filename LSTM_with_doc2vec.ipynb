{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM With doc2Vec\n",
    "\n",
    "Transform each basket into a vector, then train a LSTM on the previous baskets to predict the next basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-2.3.0.tar.gz (17.2MB)\n",
      "\u001b[K    100% |################################| 17.2MB 86kB/s eta 0:00:011  29% |#########                       | 5.0MB 5.9MB/s eta 0:00:03    30% |#########                       | 5.3MB 5.4MB/s eta 0:00:03    38% |############                    | 6.7MB 4.6MB/s eta 0:00:03    56% |##################              | 9.7MB 5.3MB/s eta 0:00:02    58% |##################              | 10.0MB 4.7MB/s eta 0:00:02    64% |####################            | 11.0MB 5.2MB/s eta 0:00:02    94% |##############################  | 16.2MB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.11.3 (from gensim)\n",
      "  Downloading numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |################################| 17.0MB 88kB/s eta 0:00:01   12% |###                             | 2.1MB 4.8MB/s eta 0:00:04    37% |############                    | 6.4MB 4.4MB/s eta 0:00:03    58% |##################              | 10.0MB 5.1MB/s eta 0:00:02    60% |###################             | 10.2MB 4.2MB/s eta 0:00:02    72% |#######################         | 12.3MB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.18.1 (from gensim)\n",
      "  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)\n",
      "\u001b[K    100% |################################| 48.2MB 31kB/s eta 0:00:011 6% |##                              | 3.1MB 4.7MB/s eta 0:00:10    7% |##                              | 3.7MB 5.2MB/s eta 0:00:09    8% |##                              | 4.0MB 5.6MB/s eta 0:00:08    11% |###                             | 5.6MB 4.4MB/s eta 0:00:10    13% |####                            | 6.7MB 5.2MB/s eta 0:00:09    14% |####                            | 6.9MB 5.2MB/s eta 0:00:08    23% |#######                         | 11.4MB 4.2MB/s eta 0:00:09    30% |#########                       | 14.6MB 5.2MB/s eta 0:00:07    47% |###############                 | 22.8MB 4.0MB/s eta 0:00:07    48% |###############                 | 23.5MB 4.8MB/s eta 0:00:06    59% |###################             | 28.7MB 5.7MB/s eta 0:00:04    62% |###################             | 30.0MB 5.2MB/s eta 0:00:04    63% |####################            | 30.5MB 6.8MB/s eta 0:00:03    65% |####################            | 31.3MB 5.0MB/s eta 0:00:04    65% |#####################           | 31.7MB 1.7MB/s eta 0:00:10    70% |######################          | 34.2MB 4.2MB/s eta 0:00:04    76% |########################        | 37.1MB 4.8MB/s eta 0:00:03    78% |#########################       | 37.9MB 5.0MB/s eta 0:00:03    79% |#########################       | 38.5MB 5.3MB/s eta 0:00:02    80% |#########################       | 38.7MB 4.9MB/s eta 0:00:02    87% |############################    | 42.3MB 5.5MB/s eta 0:00:02    91% |#############################   | 44.0MB 4.9MB/s eta 0:00:01    97% |############################### | 47.1MB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart_open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.3.tar.gz\n",
      "Collecting boto>=2.32 (from smart_open>=1.2.1->gensim)\n",
      "  Downloading boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |################################| 1.4MB 688kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bz2file (from smart_open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Collecting requests (from smart_open>=1.2.1->gensim)\n",
      "  Downloading requests-2.18.2-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |################################| 92kB 5.0MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->smart_open>=1.2.1->gensim)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 3.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->smart_open>=1.2.1->gensim)\n",
      "  Downloading certifi-2017.4.17-py2.py3-none-any.whl (375kB)\n",
      "\u001b[K    100% |################################| 378kB 936kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.23,>=1.21.1 (from requests->smart_open>=1.2.1->gensim)\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |################################| 133kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.6,>=2.5 (from requests->smart_open>=1.2.1->gensim)\n",
      "  Downloading idna-2.5-py2.py3-none-any.whl (55kB)\n",
      "\u001b[K    100% |################################| 61kB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gensim, smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for gensim ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2e/8a/18/256da0f9fb32a7a4971a5123935ad5f86c22502cebb595a1e8\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b0/81/ad/856aade935fceaab491a800ec4de58edb8642afa4c4ba91a00\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built gensim smart-open bz2file\n",
      "Installing collected packages: numpy, scipy, boto, bz2file, chardet, certifi, urllib3, idna, requests, smart-open, gensim\n",
      "  Found existing installation: numpy 1.11.3\n",
      "    Uninstalling numpy-1.11.3:\n",
      "      Successfully uninstalled numpy-1.11.3\n",
      "  Found existing installation: scipy 0.18.1\n",
      "    Uninstalling scipy-0.18.1:\n",
      "      Successfully uninstalled scipy-0.18.1\n",
      "  Found existing installation: boto 2.45.0\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (boto) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling boto-2.45.0:\n",
      "      Successfully uninstalled boto-2.45.0\n",
      "  Found existing installation: chardet 2.3.0\n",
      "    Uninstalling chardet-2.3.0:\n",
      "      Successfully uninstalled chardet-2.3.0\n",
      "  Found existing installation: idna 2.2\n",
      "    Uninstalling idna-2.2:\n",
      "      Successfully uninstalled idna-2.2\n",
      "  Found existing installation: requests 2.12.4\n",
      "    Uninstalling requests-2.12.4:\n",
      "      Successfully uninstalled requests-2.12.4\n",
      "Successfully installed boto-2.48.0 bz2file-0.98 certifi-2017.4.17 chardet-3.0.4 gensim-2.3.0 idna-2.5 numpy-1.13.1 requests-2.18.2 scipy-0.19.1 smart-open-1.5.3 urllib3-1.22\n"
     ]
    }
   ],
   "source": [
    "# Install gensim\n",
    "! pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOrders(lowerlimit, upperlimit):\n",
    "    \n",
    "    conn = sqlite3.connect(\"instacart.db\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get final order\n",
    "    cur.execute(\"SELECT B.user_id as user_id, A.order_id as order_id, \"\n",
    "                \" A.product_id as product_id, B.order_number \"\n",
    "                \"FROM products_train A INNER JOIN orders B \"\n",
    "                \" ON A.order_id = B.order_id \"\n",
    "                \"WHERE A.order_id % 100 >= \" + str(lowerlimit) + \\\n",
    "                \" AND A.order_id % 100 <= \" + str(upperlimit) + \";\")\n",
    "    train_order = np.array(cur.fetchall())\n",
    "    \n",
    "    # Get all prior orders\n",
    "    cur.execute( \\\n",
    "        \"SELECT D.user_id as user_id, \"\n",
    "        \"  D.order_id as order_id, \"\n",
    "        \"  D.order_number as order_number, \"\n",
    "        \"  C.product_id as product_id \"\n",
    "        \"FROM products_prior C INNER JOIN ( \"\n",
    "        \"  SELECT DISTINCT A.user_id as user_id,\"\n",
    "        \"    A.order_id as order_id, A.order_number as order_number \"\n",
    "        \"  FROM orders A INNER JOIN ( \"\n",
    "        \"    SELECT DISTINCT user_id FROM orders \"\n",
    "        \"    WHERE eval_set = 'train' \"\n",
    "        \"      AND order_id % 100 >= \" + str(lowerlimit) + \n",
    "        \"      AND order_id % 100 <= \" + str(upperlimit) +\n",
    "        \"    ) B ON A.user_id = B.user_id WHERE A.eval_set = 'prior' \"\n",
    "        \") D ON C.order_id = D.order_id;\")\n",
    "    prior_orders = np.array(cur.fetchall())\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return train_order, prior_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14458765, 4)\n",
      "(968968, 4)\n"
     ]
    }
   ],
   "source": [
    "y, x = getOrders(0, 69)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert purchases to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "order_docs = []\n",
    "analyzed_order = namedtuple('AnalyzedDocument', 'words tags')\n",
    "\n",
    "# both x and y are of the format: [user_id, order_id, product_id, order_number]\n",
    "# So for each distinct order_id, make a space delimited list of words\n",
    "\n",
    "order_dict = dict()\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    try:\n",
    "        order_dict[x[i][1]] += \" \"\n",
    "        order_dict[x[i][1]] += str(x[i][2])\n",
    "    except:\n",
    "        order_dict[x[i][1]] = str(x[i][2])\n",
    "        \n",
    "for order in order_dict:\n",
    "    words = order_dict[order].split()\n",
    "    tags = [order]\n",
    "    order_docs.append(analyzed_order(words, tags))\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "#model.docvecs[0]\n",
    "#model.docvecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "doc2vec_model = doc2vec.Doc2Vec(order_docs, size = 100, window = 300, min_count = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Embedding\n",
    "\n",
    "model is an embedding of all baskets. Let's test how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49302 11109 10246 49683 43633 13176 47209 22035'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab order number 1, look at products\n",
    "order_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06293377 -0.02151527 -0.00064242 -0.00924427 -0.02898256 -0.01243152\n",
      "  0.02328276 -0.00991685 -0.03488562  0.00587283 -0.06857688 -0.04181214\n",
      " -0.01552221  0.04018221 -0.000702    0.02179286 -0.01139843 -0.02207963\n",
      " -0.0051099   0.05599847 -0.02450138  0.02682662  0.00958188  0.00421782\n",
      " -0.02438697  0.00845574  0.00358347  0.01389023  0.04696041 -0.02785262\n",
      " -0.00032243  0.00817654  0.01316404  0.03014762  0.00947701  0.00023515\n",
      "  0.05599121  0.00496916 -0.04433753 -0.03634056  0.01820436  0.01616696\n",
      " -0.01494499 -0.03214792 -0.03879462  0.00453568  0.00326993 -0.0028774\n",
      "  0.02175377  0.04297222  0.00377262  0.04014904 -0.00166964 -0.02475531\n",
      "  0.02314722 -0.06379671 -0.04274622 -0.02753814  0.03868269  0.00157445\n",
      " -0.01569884 -0.05873756  0.02092193 -0.02124652  0.00322498 -0.01677983\n",
      "  0.03195389 -0.0124905  -0.01520837  0.02523862  0.0098991  -0.00233243\n",
      "  0.0370995   0.0049514  -0.03105518 -0.02719858  0.02964512 -0.02753293\n",
      "  0.02012725 -0.004572    0.00586819 -0.02517196  0.03925277  0.02496191\n",
      "  0.06007358  0.00920979 -0.03390762  0.01289168 -0.00064249  0.00097212\n",
      " -0.01401291 -0.00601675 -0.01483911 -0.02933328  0.00592861  0.01531572\n",
      "  0.00471501 -0.04711659  0.01925936  0.0055382 ]\n"
     ]
    }
   ],
   "source": [
    "# Have the model predict the vector for order_id = 1\n",
    "order1_vector = doc2vec_model.infer_vector(order_dict[1].split())\n",
    "print(order1_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(364052, 0.8343102931976318)]\n"
     ]
    }
   ],
   "source": [
    "# Find the five most similar items to order_id = 1\n",
    "print(doc2vec_model.docvecs.most_similar([order1_vector], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44632 21709 49191 7389 16440 13176\n"
     ]
    }
   ],
   "source": [
    "# see what products are in 3122701\n",
    "print(order_dict[3122701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46692 47766 13176 21616 33279\n"
     ]
    }
   ],
   "source": [
    "# What produts are in order_id = 1186825\n",
    "print(order_dict[1186825])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM Model with Basket Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create and fit the LSTM network\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(300, stateful=True, batch_input_shape=(500, 5, 100))) # input_shape=(timesteps, data_dim)\n",
    "LSTM_model.add(Dense(100))\n",
    "LSTM_model.add(Dense(100))\n",
    "LSTM_model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data in batches of 64 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch 0 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2252: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0010\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0010\n",
      "Epoch 12/15\n",
      "0s - loss: 9.9590e-04\n",
      "Epoch 13/15\n",
      "0s - loss: 9.7828e-04\n",
      "Epoch 14/15\n",
      "0s - loss: 9.5921e-04\n",
      "Epoch 15/15\n",
      "0s - loss: 9.5183e-04\n",
      "Starting batch 1 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0011\n",
      "Starting batch 2 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0020\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0013\n",
      "Starting batch 3 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0021\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0010\n",
      "Starting batch 4 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0011\n",
      "Starting batch 5 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0010\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0010\n",
      "Starting batch 6 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0012\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0011\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0011\n",
      "Starting batch 7 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0020\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0014\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0013\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0013\n",
      "Starting batch 8 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0024\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0023\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0022\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0021\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0020\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0016\n",
      "Starting batch 9 of ten.\n",
      "--------------------------------\n",
      "Working on order 0 of this batch...\n",
      "Working on order 100 of this batch...\n",
      "Working on order 200 of this batch...\n",
      "Working on order 300 of this batch...\n",
      "Working on order 400 of this batch...\n",
      "Epoch 1/15\n",
      "0s - loss: 0.0022\n",
      "Epoch 2/15\n",
      "0s - loss: 0.0021\n",
      "Epoch 3/15\n",
      "0s - loss: 0.0021\n",
      "Epoch 4/15\n",
      "0s - loss: 0.0020\n",
      "Epoch 5/15\n",
      "0s - loss: 0.0019\n",
      "Epoch 6/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 7/15\n",
      "0s - loss: 0.0018\n",
      "Epoch 8/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 9/15\n",
      "0s - loss: 0.0017\n",
      "Epoch 10/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 11/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 12/15\n",
      "0s - loss: 0.0016\n",
      "Epoch 13/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 14/15\n",
      "0s - loss: 0.0015\n",
      "Epoch 15/15\n",
      "0s - loss: 0.0015\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print(\"Starting batch \" + str(n) + \" of ten.\")\n",
    "    print(\"--------------------------------\")\n",
    "    # Delete x_train / y_train from last iteration, may fail if this is first time through\n",
    "    try:\n",
    "        del y_train\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        del x_train\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # i iterates over 500 orders \n",
    "    for i in range(500):\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print (\"Working on order \" + str(i) + \" of this batch...\")\n",
    "        # x and y have format: [user_id, order_id, product_id, order_number]\n",
    "\n",
    "        # for each order in y, make the y_train array using order_dict\n",
    "        y_temp = doc2vec_model.infer_vector(order_dict[y[(500*n) + i][1]].split())\n",
    "\n",
    "        y_temp2 = np.expand_dims(y_temp, axis=0)\n",
    "        try:\n",
    "            y_train = np.append(y_train, y_temp2, axis=0)\n",
    "        except NameError:\n",
    "            y_train = y_temp2\n",
    "\n",
    "\n",
    "        final_order_num = y[(500*n) + i][3]\n",
    "        final_order_user_id = y[(500*n) + i][0]\n",
    "\n",
    "        # grab this user's prior orders\n",
    "        prior_orders = x[np.where(x[:,0] == final_order_user_id)]\n",
    "\n",
    "        cur_order_string = ''\n",
    "\n",
    "        try:\n",
    "            del cur_order_x_train\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # go grab the products for each prior order\n",
    "        # dropping zero index, since we want to subtract at least one from final order num\n",
    "        for j in range(1, 6):\n",
    "\n",
    "            # Reset \n",
    "            del cur_order_string\n",
    "\n",
    "            # Make string of products for current order\n",
    "            if final_order_num - j > 0:\n",
    "                for product in [prod for prod in prior_orders if prod[2] == final_order_num - j]:\n",
    "                    try:\n",
    "                        cur_order_string += \" \"\n",
    "                        cur_order_string += str(product[3])\n",
    "                    except:\n",
    "                        cur_order_string = str(product[3])\n",
    "            else:\n",
    "                cur_order_string = \"\"\n",
    "\n",
    "            # Turn string into vector\n",
    "            cur_order_x_temp = doc2vec_model.infer_vector(cur_order_string.split())\n",
    "\n",
    "            # Expand vectors to include empty timestep dimension\n",
    "            cur_order_x_temp = np.expand_dims(cur_order_x_temp, axis=0)\n",
    "\n",
    "            # cur_order_x_temp is the vector of this time step's basket. \n",
    "            # add it to the tensor for cur_order_x_train\n",
    "            # Notice, cur_order_x_temp is added on left, we are going backwards in timesteps\n",
    "            try:\n",
    "                cur_order_x_train = np.append(cur_order_x_temp, cur_order_x_train, axis=0)\n",
    "            except:\n",
    "                cur_order_x_train = cur_order_x_temp\n",
    "\n",
    "        # ASSERT: cur_order_x_train has five timesteps now times 100 vectors, shape = (5, 100)\n",
    "\n",
    "        # Expand full order to batch dimension\n",
    "        cur_order_x_train = np.expand_dims(cur_order_x_train, axis=0)\n",
    "\n",
    "        # Create x_train or add to it \n",
    "        try: \n",
    "            x_train = np.append(x_train, cur_order_x_train, axis=0)\n",
    "        except:\n",
    "            x_train = cur_order_x_train\n",
    "\n",
    "    # Done creating batch, time to train\n",
    "    LSTM_model.fit(x_train, y_train, epochs=15, batch_size=x_train.shape[0], verbose=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the validation orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab validation orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get validation orders:\n",
    "y_validation, x_validation = getOrders(70, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change LSTM Model to accept one order at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-define model\n",
    "LSTM_model_predict = Sequential()\n",
    "LSTM_model_predict.add(LSTM(300, stateful=True, batch_input_shape=(1, 5, 100))) # input_shape=(timesteps, data_dim)\n",
    "LSTM_model_predict.add(Dense(100))\n",
    "LSTM_model_predict.add(Dense(100))\n",
    "\n",
    "# copy weights\n",
    "old_weights = LSTM_model.get_weights()\n",
    "LSTM_model_predict.set_weights(old_weights)\n",
    "\n",
    "# compile model\n",
    "LSTM_model_predict.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Function to predict for a given order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to predict for an offer:\n",
    "# # x_valid is a numpy array of [user_id, order_id, product_id, order_number]\n",
    "# # ...but only for a single user / final validation order\n",
    "# user_id = customer we are predicting a final order for\n",
    "# final_order_number = order_number of the order we are predicting\n",
    "def makePrediction(x_valid, final_order_number):\n",
    "    \n",
    "\n",
    "    # go grab the products for each prior order\n",
    "    # dropping zero index, since we want to subtract at least one from final order num\n",
    "    for j in range(1, 6):\n",
    "\n",
    "        # Reset order string, which will error if this is the first loop \n",
    "        try:\n",
    "            del cur_order_string\n",
    "        except: \n",
    "            pass        \n",
    "\n",
    "        # Make string of products for current order\n",
    "        if final_order_number - j > 0:\n",
    "            for product in [prod for prod in x_valid if prod[2] == final_order_number - j]:\n",
    "                try:\n",
    "                    cur_order_string += \" \"\n",
    "                    cur_order_string += str(product[3])\n",
    "                except:\n",
    "                    cur_order_string = str(product[3])\n",
    "        else:\n",
    "            cur_order_string = \"\"\n",
    "\n",
    "        # Turn string into vector\n",
    "        cur_order_x_temp = doc2vec_model.infer_vector(cur_order_string.split())\n",
    "\n",
    "        # Expand vectors to include empty timestep dimension\n",
    "        cur_order_x_temp = np.expand_dims(cur_order_x_temp, axis=0)\n",
    "\n",
    "        # cur_order_x_temp is the vector of this time step's basket. \n",
    "        # add it to the tensor for cur_order_x_train\n",
    "        # Notice, cur_order_x_temp is added on left, we are going backwards in timesteps\n",
    "        try:\n",
    "            cur_order_x_train = np.append(cur_order_x_temp, cur_order_x_train, axis=0)\n",
    "        except:\n",
    "            cur_order_x_train = cur_order_x_temp\n",
    "            \n",
    "    # Expand to 3 dimensions\n",
    "    cur_order_x_train = np.expand_dims(cur_order_x_train, axis=0)\n",
    "    \n",
    "    # Make Prediction\n",
    "    prediction = LSTM_model_predict.predict(cur_order_x_train)\n",
    "    \n",
    "    # Find closest basket \n",
    "    #closest_basket = doc2vec_model.docvecs.most_similar([prediction], topn=1)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81609, 4)\n",
      "(1224500, 4)\n"
     ]
    }
   ],
   "source": [
    "print(y_validation.shape)\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cur_user = y_validation[i][0]\n",
    "cur_user_order_num = y_validation[i][3]\n",
    "cur_user_prior_orders = x_validation[np.where(x_validation[:,0] == cur_user)]\n",
    "cur_user_input = makePrediction(cur_user_prior_orders, cur_user_order_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1720460, 0.7618535757064819)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(cur_user_input.flatten())\n",
    "doc2vec_model.docvecs.most_similar([cur_user_input.flatten()], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38557 20995 13176 47766 37646 46969 21137 21174 260 24184 16759 46049 1468 40706 22035\n"
     ]
    }
   ],
   "source": [
    "print(order_dict[1720460])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[182389    170  18394      7]\n",
      " [182389    170  37766      7]\n",
      " [182389    170  13176      7]\n",
      " [182389    170   6236      7]\n",
      " [182389    170   5077      7]\n",
      " [182389    170   8153      7]\n",
      " [182389    170  43772      7]\n",
      " [182389    170  25591      7]\n",
      " [182389    170  34582      7]\n",
      " [182389    170  49593      7]\n",
      " [182389    170  15093      7]\n",
      " [182389    170  43841      7]\n",
      " [182389    170  21137      7]\n",
      " [182389    170  40354      7]\n",
      " [182389    170  17794      7]\n",
      " [182389    170  11182      7]\n",
      " [182389    170  39190      7]\n",
      " [ 77529    473  20082      7]\n",
      " [ 77529    473  24852      7]\n",
      " [ 77529    473  47144      7]\n",
      " [ 77529    473  36441      7]\n",
      " [ 77529    473  12206      7]\n",
      " [ 77529    473   4034      7]\n",
      " [ 77529    473  30573      7]\n",
      " [ 77529    473  42404      7]\n",
      " [ 27650    774  47482     25]\n",
      " [ 27650    774  43335     25]\n",
      " [ 27650    774  16108     25]\n",
      " [ 10125   1275   6046      5]\n",
      " [ 10125   1275  48679      5]]\n"
     ]
    }
   ],
   "source": [
    "print(y_validation[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
